# -*- coding: utf-8 -*-
"""Copy of UAS_RNN_V3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1birulSrMx4voxU2vAb8puixDfqGuxfCM
"""

!pip install numpy pandas tensorflow gensim scikit-learn matplotlib seaborn wikiextractor
!pip install tensorflow --upgrade

# DataFrame
import pandas as pd

# Matplotlib
import matplotlib.pyplot as plt
import matplotlib

# Scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score, precision_score, f1_score

# Keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, load_model, Model
from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM
from tensorflow.keras.optimizers import RMSprop

# Word2Vec
import gensim
from gensim.models import Word2Vec

# Utility
import re
import numpy as np
import os
import time
from collections import Counter
import string
from nltk.tokenize import word_tokenize
import seaborn as sns

# Widgets (optional, for interactive notebooks)
from ipywidgets import widgets

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/data_UAS_KCB/data_train_V3.csv')
data.head()

# Fungsi untuk membersihkan tweet
slang_dict = {"@": "di", "abis": "habis", "ad": "ada", "adlh": "adalah", "afaik": "as far as i know", "ahaha": "haha", "aj": "saja", "ajep-ajep": "dunia gemerlap", "ak": "saya", "akika": "aku", "akkoh": "aku", "akuwh": "aku", "alay": "norak", "alow": "halo", "ambilin": "ambilkan", "ancur": "hancur", "anjrit": "anjing", "anter": "antar", "ap2": "apa-apa", "apasih": "apa sih", "apes": "sial", "aps": "apa", "aq": "saya", "aquwh": "aku", "asbun": "asal bunyi", "aseekk": "asyik", "asekk": "asyik", "asem": "asam", "aspal": "asli tetapi palsu", "astul": "asal tulis", "ato": "atau", "au ah": "tidak mau tahu", "awak": "saya", "ay": "sayang", "ayank": "sayang", "b4": "sebelum", "bakalan": "akan", "bandes": "bantuan desa", "bangedh": "banget", "banpol": "bantuan polisi", "banpur": "bantuan tempur", "basbang": "basi", "bcanda": "bercanda", "bdg": "bandung", "begajulan": "nakal", "beliin": "belikan", "bencong": "banci", "bentar": "sebentar", "ber3": "bertiga", "beresin": "membereskan", "bete": "bosan", "beud": "banget", "bg": "abang", "bgmn": "bagaimana", "bgt": "banget", "bijimane": "bagaimana", "bintal": "bimbingan mental", "bkl": "akan", "bknnya": "bukannya", "blegug": "bodoh", "blh": "boleh", "bln": "bulan", "blum": "belum", "bnci": "benci", "bnran": "yang benar", "bodor": "lucu", "bokap": "ayah", "boker": "buang air besar", "bokis": "bohong", "boljug": "boleh juga", "bonek": "bocah nekat", "boyeh": "boleh", "br": "baru", "brg": "bareng", "bro": "saudara laki-laki", "bru": "baru", "bs": "bisa", "bsen": "bosan", "bt": "buat", "btw": "ngomong-ngomong", "buaya": "tidak setia", "bubbu": "tidur", "bubu": "tidur", "bumil": "ibu hamil", "bw": "bawa", "bwt": "buat", "byk": "banyak", "byrin": "bayarkan", "cabal": "sabar", "cadas": "keren", "calo": "makelar", "can": "belum", "capcus": "pergi", "caper": "cari perhatian", "ce": "cewek", "cekal": "cegah tangkal", "cemen": "penakut", "cengengesan": "tertawa", "cepet": "cepat", "cew": "cewek", "chuyunk": "sayang", "cimeng": "ganja", "cipika cipiki": "cium pipi kanan cium pipi kiri", "ciyh": "sih", "ckepp": "cakep", "ckp": "cakep", "cmiiw": "correct me if i'm wrong", "cmpur": "campur", "cong": "banci", "conlok": "cinta lokasi", "cowwyy": "maaf", "cp": "siapa", "cpe": "capek", "cppe": "capek", "cucok": "cocok", "cuex": "cuek", "cumi": "Cuma miscall", "cups": "culun", "curanmor": "pencurian kendaraan bermotor", "curcol": "curahan hati colongan", "cwek": "cewek", "cyin": "cinta", "d": "di", "dah": "deh", "dapet": "dapat", "de": "adik", "dek": "adik", "demen": "suka", "deyh": "deh", "dgn": "dengan", "diancurin": "dihancurkan", "dimaafin": "dimaafkan", "dimintak": "diminta", "disono": "di sana", "dket": "dekat", "dkk": "dan kawan-kawan", "dll": "dan lain-lain", "dlu": "dulu", "dngn": "dengan", "dodol": "bodoh", "doku": "uang", "dongs": "dong", "dpt": "dapat", "dri": "dari", "drmn": "darimana", "drtd": "dari tadi", "dst": "dan seterusnya", "dtg": "datang", "duh": "aduh", "duren": "durian", "ed": "edisi", "egp": "emang gue pikirin", "eke": "aku", "elu": "kamu", "emangnya": "memangnya", "emng": "memang", "endak": "tidak", "enggak": "tidak", "envy": "iri", "ex": "mantan", "fax": "facsimile", "fifo": "first in first out", "folbek": "follow back", "fyi": "sebagai informasi", "gaada": "tidak ada uang", "gag": "tidak", "gaje": "tidak jelas", "gak papa": "tidak apa-apa", "gan": "juragan", "gaptek": "gagap teknologi", "gatek": "gagap teknologi", "gawe": "kerja", "gbs": "tidak bisa", "gebetan": "orang yang disuka", "geje": "tidak jelas", "gepeng": "gelandangan dan pengemis", "ghiy": "lagi", "gile": "gila", "gimana": "bagaimana", "gino": "gigi nongol", "githu": "gitu", "gj": "tidak jelas", "gmana": "bagaimana", "gn": "begini", "goblok": "bodoh", "golput": "golongan putih", "gowes": "mengayuh sepeda", "gpny": "tidak punya", "gr": "gede rasa", "gretongan": "gratisan", "gtau": "tidak tahu", "gua": "saya", "guoblok": "goblok", "gw": "saya", "ha": "tertawa", "haha": "tertawa", "hallow": "halo", "hankam": "pertahanan dan keamanan", "hehe": "he", "helo": "halo", "hey": "hai", "hlm": "halaman", "hny": "hanya", "hoax": "isu bohong", "hr": "hari", "hrus": "harus", "hubdar": "perhubungan darat", "huff": "mengeluh", "hum": "rumah", "humz": "rumah", "ilang": "hilang", "ilfil": "tidak suka", "imho": "in my humble opinion", "imoetz": "imut", "item": "hitam", "itungan": "hitungan", "iye": "iya", "ja": "saja", "jadiin": "jadi", "jaim": "jaga image", "jayus": "tidak lucu", "jdi": "jadi", "jem": "jam", "jga": "juga", "jgnkan": "jangankan", "jir": "anjing", "jln": "jalan", "jomblo": "tidak punya pacar", "jubir": "juru bicara", "jutek": "galak", "k": "ke", "kab": "kabupaten", "kabor": "kabur", "kacrut": "kacau", "kadiv": "kepala divisi", "kagak": "tidak", "kalo": "kalau", "kampret": "sialan", "kamtibmas": "keamanan dan ketertiban masyarakat", "kamuwh": "kamu", "kanwil": "kantor wilayah", "karna": "karena", "kasubbag": "kepala subbagian", "katrok": "kampungan", "kayanya": "kayaknya", "kbr": "kabar", "kdu": "harus", "kec": "kecamatan", "kejurnas": "kejuaraan nasional", "kekeuh": "keras kepala", "kel": "kelurahan", "kemaren": "kemarin", "kepengen": "mau", "kepingin": "mau", "kepsek": "kepala sekolah", "kesbang": "kesatuan bangsa", "kesra": "kesejahteraan rakyat", "ketrima": "diterima", "kgiatan": "kegiatan", "kibul": "bohong", "kimpoi": "kawin", "kl": "kalau", "klianz": "kalian", "kloter": "kelompok terbang", "klw": "kalau", "km": "kamu", "kmps": "kampus", "kmrn": "kemarin", "knal": "kenal", "knp": "kenapa", "kodya": "kota madya", "komdis": "komisi disiplin", "komsov": "komunis sovyet", "kongkow": "kumpul bareng teman-teman", "kopdar": "kopi darat", "korup": "korupsi", "kpn": "kapan", "krenz": "keren", "krm": "kirim", "kt": "kita", "ktmu": "ketemu", "ktr": "kantor", "kuper": "kurang pergaulan", "kw": "imitasi", "kyk": "seperti", "la": "lah", "lam": "salam", "lamp": "lampiran", "lanud": "landasan udara", "latgab": "latihan gabungan", "lebay": "berlebihan", "leh": "boleh", "lelet": "lambat", "lemot": "lambat", "lgi": "lagi", "lgsg": "langsung", "liat": "lihat", "litbang": "penelitian dan pengembangan", "lmyn": "lumayan", "lo": "kamu", "loe": "kamu", "lola": "lambat berfikir", "louph": "cinta", "low": "kalau", "lp": "lupa", "luber": "langsung, umum, bebas, dan rahasia", "luchuw": "lucu", "lum": "belum", "luthu": "lucu", "lwn": "lawan", "maacih": "terima kasih", "mabal": "bolos", "macem": "macam", "macih": "masih", "maem": "makan", "magabut": "makan gaji buta", "maho": "homo", "mak jang": "kaget", "maksain": "memaksa", "malem": "malam", "mam": "makan", "maneh": "kamu", "maniez": "manis", "mao": "mau", "masukin": "masukkan", "melu": "ikut", "mepet": "dekat sekali", "mgu": "minggu", "migas": "minyak dan gas bumi", "mikol": "minuman beralkohol", "miras": "minuman keras", "mlah": "malah", "mngkn": "mungkin", "mo": "mau", "mokad": "mati", "moso": "masa", "mpe": "sampai", "msk": "masuk", "mslh": "masalah", "mt": "makan teman", "mubes": "musyawarah besar", "mulu": "melulu", "mumpung": "selagi", "munas": "musyawarah nasional", "muntaber": "muntah dan berak", "musti": "mesti", "muupz": "maaf", "mw": "now watching", "n": "dan", "nanam": "menanam", "nanya": "bertanya", "napa": "kenapa", "napi": "narapidana", "napza": "narkotika, alkohol, psikotropika, dan zat adiktif ", "narkoba": "narkotika, psikotropika, dan obat terlarang", "nasgor": "nasi goreng", "nda": "tidak", "ndiri": "sendiri", "ne": "ini", "nekolin": "neokolonialisme", "nembak": "menyatakan cinta", "ngabuburit": "menunggu berbuka puasa", "ngaku": "mengaku", "ngambil": "mengambil", "nganggur": "tidak punya pekerjaan", "ngapah": "kenapa", "ngaret": "terlambat", "ngasih": "memberikan", "ngebandel": "berbuat bandel", "ngegosip": "bergosip", "ngeklaim": "mengklaim", "ngeksis": "menjadi eksis", "ngeles": "berkilah", "ngelidur": "menggigau", "ngerampok": "merampok", "ngga": "tidak", "ngibul": "berbohong", "ngiler": "mau", "ngiri": "iri", "ngisiin": "mengisikan", "ngmng": "bicara", "ngomong": "bicara", "ngubek2": "mencari-cari", "ngurus": "mengurus", "nie": "ini", "nih": "ini", "niyh": "nih", "nmr": "nomor", "nntn": "nonton", "nobar": "nonton bareng", "np": "now playing", "ntar": "nanti", "ntn": "nonton", "numpuk": "bertumpuk", "nutupin": "menutupi", "nyari": "mencari", "nyekar": "menyekar", "nyicil": "mencicil", "nyoblos": "mencoblos", "nyokap": "ibu", "ogah": "tidak mau", "ol": "online", "ongkir": "ongkos kirim", "oot": "out of topic", "org2": "orang-orang", "ortu": "orang tua", "otda": "otonomi daerah", "otw": "on the way, sedang di jalan", "pacal": "pacar", "pake": "pakai", "pala": "kepala", "pansus": "panitia khusus", "parpol": "partai politik", "pasutri": "pasangan suami istri", "pd": "pada", "pede": "percaya diri", "pelatnas": "pemusatan latihan nasional", "pemda": "pemerintah daerah", "pemkot": "pemerintah kota", "pemred": "pemimpin redaksi", "penjas": "pendidikan jasmani", "perda": "peraturan daerah", "perhatiin": "perhatikan", "pesenan": "pesanan", "pgang": "pegang", "pi": "tapi", "pilkada": "pemilihan kepala daerah", "pisan": "sangat", "pk": "penjahat kelamin", "plg": "paling", "pmrnth": "pemerintah", "polantas": "polisi lalu lintas", "ponpes": "pondok pesantren", "pp": "pulang pergi", "prg": "pergi", "prnh": "pernah", "psen": "pesan", "pst": "pasti", "pswt": "pesawat", "pw": "posisi nyaman", "qmu": "kamu", "rakor": "rapat koordinasi", "ranmor": "kendaraan bermotor", "re": "reply", "ref": "referensi", "rehab": "rehabilitasi", "rempong": "sulit", "repp": "balas", "restik": "reserse narkotika", "rhs": "rahasia", "rmh": "rumah", "ru": "baru", "ruko": "rumah toko", "rusunawa": "rumah susun sewa", "ruz": "terus", "saia": "saya", "salting": "salah tingkah", "sampe": "sampai", "samsek": "sama sekali", "sapose": "siapa", "satpam": "satuan pengamanan", "sbb": "sebagai berikut", "sbh": "sebuah", "sbnrny": "sebenarnya", "scr": "secara", "sdgkn": "sedangkan", "sdkt": "sedikit", "se7": "setuju", "sebelas dua belas": "mirip", "sembako": "sembilan bahan pokok", "sempet": "sempat", "sendratari": "seni drama tari", "sgt": "sangat", "shg": "sehingga", "siech": "sih", "sikon": "situasi dan kondisi", "sinetron": "sinema elektronik", "siramin": "siramkan", "sj": "saja", "skalian": "sekalian", "sklh": "sekolah", "skt": "sakit", "slesai": "selesai", "sll": "selalu", "slma": "selama", "slsai": "selesai", "smpt": "sempat", "smw": "semua", "sndiri": "sendiri", "soljum": "sholat jumat", "songong": "sombong", "sory": "maaf", "sosek": "sosial-ekonomi", "sotoy": "sok tahu", "spa": "siapa", "sppa": "siapa", "spt": "seperti", "srtfkt": "sertifikat", "stiap": "setiap", "stlh": "setelah", "suk": "masuk", "sumpek": "sempit", "syg": "sayang", "t4": "tempat", "tajir": "kaya", "tau": "tahu", "taw": "tahu", "td": "tadi", "tdk": "tidak", "teh": "kakak perempuan", "telat": "terlambat", "telmi": "telat berpikir", "temen": "teman", "tengil": "menyebalkan", "tepar": "terkapar", "tggu": "tunggu", "tgu": "tunggu", "thankz": "terima kasih", "thn": "tahun", "tilang": "bukti pelanggaran", "tipiwan": "TvOne", "tks": "terima kasih", "tlp": "telepon", "tls": "tulis", "tmbah": "tambah", "tmen2": "teman-teman", "tmpah": "tumpah", "tmpt": "tempat", "tngu": "tunggu", "tnyta": "ternyata", "tokai": "tai", "toserba": "toko serba ada", "tpi": "tapi", "trdhulu": "terdahulu", "trima": "terima kasih", "trm": "terima", "trs": "terus", "trutama": "terutama", "ts": "penulis", "tst": "tahu sama tahu", "ttg": "tentang", "tuch": "tuh", "tuir": "tua", "tw": "tahu", "u": "kamu", "ud": "sudah", "udah": "sudah", "ujg": "ujung", "ul": "ulangan", "unyu": "lucu", "uplot": "unggah", "urang": "saya", "usah": "perlu", "utk": "untuk", "valas": "valuta asing", "w/": "dengan", "wadir": "wakil direktur", "wamil": "wajib militer", "warkop": "warung kopi", "warteg": "warung tegal", "wat": "buat", "wkt": "waktu", "wtf": "what the fuck", "xixixi": "tertawa", "ya": "iya", "yap": "iya", "yaudah": "ya sudah", "yawdah": "ya sudah", "yg": "yang", "yl": "yang lain", "yo": "iya", "yowes": "ya sudah", "yup": "iya", "7an": "tujuan", "ababil": "abg labil", "acc": "accord", "adlah": "adalah", "adoh": "aduh", "aha": "tertawa", "aing": "saya", "aja": "saja", "ajj": "saja", "aka": "dikenal juga sebagai", "akko": "aku", "akku": "aku", "akyu": "aku", "aljasa": "asal jadi saja", "ama": "sama", "ambl": "ambil", "anjir": "anjing", "ank": "anak", "ap": "apa", "apaan": "apa", "ape": "apa", "aplot": "unggah", "apva": "apa", "aqu": "aku", "asap": "sesegera mungkin", "aseek": "asyik", "asek": "asyik", "aseknya": "asyiknya", "asoy": "asyik", "astrojim": "astagfirullahaladzim", "ath": "kalau begitu", "atuh": "kalau begitu", "ava": "avatar", "aws": "awas", "ayang": "sayang", "ayok": "ayo", "bacot": "banyak bicara", "bales": "balas", "bangdes": "pembangunan desa", "bangkotan": "tua", "banpres": "bantuan presiden", "bansarkas": "bantuan sarana kesehatan", "bazis": "badan amal, zakat, infak, dan sedekah", "bcoz": "karena", "beb": "sayang", "bejibun": "banyak", "belom": "belum", "bener": "benar", "ber2": "berdua", "berdikari": "berdiri di atas kaki sendiri", "bet": "banget", "beti": "beda tipis", "beut": "banget", "bgd": "banget", "bgs": "bagus", "bhubu": "tidur", "bimbuluh": "bimbingan dan penyuluhan", "bisi": "kalau-kalau", "bkn": "bukan", "bl": "beli", "blg": "bilang", "blm": "belum", "bls": "balas", "bnchi": "benci", "bngung": "bingung", "bnyk": "banyak", "bohay": "badan aduhai", "bokep": "porno", "bokin": "pacar", "bole": "boleh", "bolot": "bodoh", "bonyok": "ayah ibu", "bpk": "bapak", "brb": "segera kembali", "brngkt": "berangkat", "brp": "berapa", "brur": "saudara laki-laki", "bsa": "bisa", "bsk": "besok", "bu_bu": "tidur", "bubarin": "bubarkan", "buber": "buka bersama", "bujubune": "luar biasa", "buser": "buru sergap", "bwhn": "bawahan", "byar": "bayar", "byr": "bayar", "c8": "chat", "cabut": "pergi", "caem": "cakep", "cama-cama": "sama-sama", "cangcut": "celana dalam", "cape": "capek", "caur": "jelek", "cekak": "tidak ada uang", "cekidot": "coba lihat", "cemplungin": "cemplungkan", "ceper": "pendek", "ceu": "kakak perempuan", "cewe": "cewek", "cibuk": "sibuk", "cin": "cinta", "ciye": "cie", "ckck": "ck", "clbk": "cinta lama bersemi kembali", "cmpr": "campur", "cnenk": "senang", "congor": "mulut", "cow": "cowok", "coz": "karena", "cpa": "siapa", "gokil": "gila", "gombal": "suka merayu", "gpl": "tidak pakai lama", "gpp": "tidak apa-apa", "gretong": "gratis", "gt": "begitu", "gtw": "tidak tahu", "gue": "saya", "guys": "teman-teman", "gws": "cepat sembuh", "haghaghag": "tertawa", "hakhak": "tertawa", "handak": "bahan peledak", "hansip": "pertahanan sipil", "hellow": "halo", "helow": "halo", "hi": "hai", "hlng": "hilang", "hnya": "hanya", "houm": "rumah", "hrs": "harus", "hubad": "hubungan angkatan darat", "hubla": "perhubungan laut", "huft": "mengeluh", "humas": "hubungan masyarakat", "idk": "saya tidak tahu", "ilfeel": "tidak suka", "imba": "jago sekali", "imoet": "imut", "info": "informasi", "itung": "hitung", "isengin": "bercanda", "iyala": "iya lah", "iyo": "iya", "jablay": "jarang dibelai", "jadul": "jaman dulu", "jancuk": "anjing", "jd": "jadi", "jdikan": "jadikan", "jg": "juga", "jgn": "jangan", "jijay": "jijik", "jkt": "jakarta", "jnj": "janji", "jth": "jatuh", "jurdil": "jujur adil", "jwb": "jawab", "ka": "kakak", "kabag": "kepala bagian", "kacian": "kasihan", "kadit": "kepala direktorat", "kaga": "tidak", "kaka": "kakak", "kamtib": "keamanan dan ketertiban", "kamuh": "kamu", "kamyu": "kamu", "kapt": "kapten", "kasat": "kepala satuan", "kasubbid": "kepala subbidang", "kau": "kamu", "kbar": "kabar", "kcian": "kasihan", "keburu": "terlanjur", "kedubes": "kedutaan besar", "kek": "seperti", "keknya": "kayaknya", "keliatan": "kelihatan", "keneh": "masih", "kepikiran": "terpikirkan", "kepo": "mau tahu urusan orang", "kere": "tidak punya uang", "kesian": "kasihan", "ketauan": "ketahuan", "keukeuh": "keras kepala", "khan": "kan", "kibus": "kaki busuk", "kk": "kakak", "klian": "kalian", "klo": "kalau", "kluarga": "keluarga", "klwrga": "keluarga", "kmari": "kemari", "kmpus": "kampus", "kn": "kan", "knl": "kenal", "knpa": "kenapa", "kog": "kok", "kompi": "komputer", "komtiong": "komunis Tiongkok", "konjen": "konsulat jenderal", "koq": "kok", "kpd": "kepada", "kptsan": "keputusan", "krik": "garing", "krn": "karena", "ktauan": "ketahuan", "ktny": "katanya", "kudu": "harus", "kuq": "kok", "ky": "seperti", "kykny": "kayanya", "laka": "kecelakaan", "lambreta": "lambat", "lansia": "lanjut usia", "lapas": "lembaga pemasyarakatan", "lbur": "libur", "lekong": "laki-laki", "lg": "lagi", "lgkp": "lengkap", "lht": "lihat", "linmas": "perlindungan masyarakat", "lmyan": "lumayan", "lngkp": "lengkap", "loch": "loh", "lol": "tertawa", "lom": "belum", "loupz": "cinta", "lowh": "kamu", "lu": "kamu", "luchu": "lucu", "luff": "cinta", "luph": "cinta", "lw": "kamu", "lwt": "lewat", "maaciw": "terima kasih", "mabes": "markas besar", "macem-macem": "macam-macam", "madesu": "masa depan suram", "maen": "main", "mahatma": "maju sehat bersama", "mak": "ibu", "makasih": "terima kasih", "malah": "bahkan", "malu2in": "memalukan", "mamz": "makan", "manies": "manis", "mantep": "mantap", "markus": "makelar kasus", "mba": "mbak", "mending": "lebih baik", "mgkn": "mungkin", "mhn": "mohon", "miker": "minuman keras", "milis": "mailing list", "mksd": "maksud", "mls": "malas", "mnt": "minta", "moge": "motor gede", "mokat": "mati", "mosok": "masa", "msh": "masih", "mskpn": "meskipun", "msng2": "masing-masing", "muahal": "mahal", "muker": "musyawarah kerja", "mumet": "pusing", "muna": "munafik", "munaslub": "musyawarah nasional luar biasa", "musda": "musyawarah daerah", "muup": "maaf", "muuv": "maaf", "nal": "kenal", "nangis": "menangis", "naon": "apa", "napol": "narapidana politik", "naq": "anak", "narsis": "bangga pada diri sendiri", "nax": "anak", "ndak": "tidak", "ndut": "gendut", "nekolim": "neokolonialisme", "nelfon": "menelepon", "ngabis2in": "menghabiskan", "ngakak": "tertawa", "ngambek": "marah", "ngampus": "pergi ke kampus", "ngantri": "mengantri", "ngapain": "sedang apa", "ngaruh": "berpengaruh", "ngawur": "berbicara sembarangan", "ngeceng": "kumpul bareng-bareng", "ngeh": "sadar", "ngekos": "tinggal di kos", "ngelamar": "melamar", "ngeliat": "melihat", "ngemeng": "bicara terus-terusan", "ngerti": "mengerti", "nggak": "tidak", "ngikut": "ikut", "nginep": "menginap", "ngisi": "mengisi", "ngmg": "bicara", "ngocol": "lucu", "ngomongin": "membicarakan", "ngumpul": "berkumpul", "ni": "ini", "nyasar": "tersesat", "nyariin": "mencari", "nyiapin": "mempersiapkan", "nyiram": "menyiram", "nyok": "ayo", "o/": "oleh", "ok": "ok", "priksa": "periksa", "pro": "profesional", "psn": "pesan", "psti": "pasti", "puanas": "panas", "qmo": "kamu", "qt": "kita", "rame": "ramai", "raskin": "rakyat miskin", "red": "redaksi", "reg": "register", "rejeki": "rezeki", "renstra": "rencana strategis", "reskrim": "reserse kriminal", "sni": "sini", "somse": "sombong sekali", "sorry": "maaf", "sosbud": "sosial-budaya", "sospol": "sosial-politik", "sowry": "maaf", "spd": "sepeda", "sprti": "seperti", "spy": "supaya", "stelah": "setelah", "subbag": "subbagian", "sumbangin": "sumbangkan", "sy": "saya", "syp": "siapa", "tabanas": "tabungan pembangunan nasional", "tar": "nanti", "taun": "tahun", "tawh": "tahu", "tdi": "tadi", "te2p": "tetap", "tekor": "rugi", "telkom": "telekomunikasi", "telp": "telepon", "temen2": "teman-teman", "tengok": "menjenguk", "terbitin": "terbitkan", "tgl": "tanggal", "thanks": "terima kasih", "thd": "terhadap", "thx": "terima kasih", "tipi": "TV", "tkg": "tukang", "tll": "terlalu", "tlpn": "telepon", "tman": "teman", "tmbh": "tambah", "tmn2": "teman-teman", "tmph": "tumpah", "tnda": "tanda", "tnh": "tanah", "togel": "toto gelap", "tp": "tapi", "tq": "terima kasih", "trgntg": "tergantung", "trims": "terima kasih", "cb": "coba", "y": "ya", "munfik": "munafik", "reklamuk": "reklamasi", "sma": "sama", "tren": "trend", "ngehe": "kesal", "mz": "mas", "analisise": "analisis", "sadaar": "sadar", "sept": "september", "nmenarik": "menarik", "zonk": "bodoh", "rights": "benar", "simiskin": "miskin", "ngumpet": "sembunyi", "hardcore": "keras", "akhirx": "akhirnya", "solve": "solusi", "watuk": "batuk", "ngebully": "intimidasi", "masy": "masyarakat", "still": "masih", "tauk": "tahu", "mbual": "bual", "tioghoa": "tionghoa", "ngentotin": "senggama", "kentot": "senggama", "faktakta": "fakta", "sohib": "teman", "rubahnn": "rubah", "trlalu": "terlalu", "nyela": "cela", "heters": "pembenci", "nyembah": "sembah", "most": "paling", "ikon": "lambang", "light": "terang", "pndukung": "pendukung", "setting": "atur", "seting": "akting", "next": "lanjut", "waspadalah": "waspada", "gantengsaya": "ganteng", "parte": "partai", "nyerang": "serang", "nipu": "tipu", "ktipu": "tipu", "jentelmen": "berani", "buangbuang": "buang", "tsangka": "tersangka", "kurng": "kurang", "ista": "nista", "less": "kurang", "koar": "teriak", "paranoid": "takut", "problem": "masalah", "tahi": "kotoran", "tirani": "tiran", "tilep": "tilap", "happy": "bahagia", "tak": "tidak", "penertiban": "tertib", "uasai": "kuasa", "mnolak": "tolak", "trending": "trend", "taik": "tahi", "wkwkkw": "tertawa", "ahokncc": "ahok", "istaa": "nista", "benarjujur": "jujur", "mgkin": "mungkin"}
def clean_text(text):
    text = text.lower()  # Lowercasing
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'@[\w]*', '', text)  # Remove mentions
    text = re.sub(r'#[\w]*', '', text)  # Remove hashtags
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation

    # Replace slang words with appropriate replacements from the slang dictionary
    text = ' '.join([slang_dict.get(word, word) for word in text.split()])  # Replace slang words
    return text

# Apply cleaning function
data['cleaned_text'] = data['Tweet'].apply(clean_text)
print(f"Jumlah data setelah preprocessing: {len(data)}")
data.head(10)

# Kolom yang akan digunakan untuk preprocessing
X = data['cleaned_text']  # Kolom Tweet berisi teks
y = data['Label']  # Kolom Label berisi sentimen

# Membagi data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Jumlah data latih: {len(X_train)}")
print(f"Jumlah data uji: {len(X_test)}")

# Hitung panjang tweet
data['tweet_length'] = data['cleaned_text'].apply(lambda x: len(x.split()))
max_len = 55

# Visualisasikan distribusi panjang tweet
plt.hist(data['tweet_length'], bins=20)
plt.title('Distribusi Panjang Tweet')
plt.xlabel('Panjang Tweet')
plt.ylabel('Frekuensi')
plt.show()
print(data['tweet_length'].describe())

# Menampilkan beberapa baris pertama dari file XML untuk memverifikasi strukturnya
with open('/content/drive/MyDrive/data_UAS_KCB/idwiki-latest-pages-articles.xml', 'r', encoding='utf-8') as file:
    for i in range(10):  # Menampilkan 10 baris pertama
        print(file.readline())

import xml.etree.ElementTree as ET
import re

# --- 1. DEFINISIKAN PATH ---
wikipedia_xml_dump_path = '/content/drive/MyDrive/data_UAS_KCB/idwiki-latest-pages-articles.xml'  # Path ke file XML Wikipedia
processed_wikipedia_corpus_file = '/content/drive/MyDrive/data_UAS_KCB/processed_wikipedia_tokenized.txt'  # Tempat untuk menyimpan korpus yang sudah diproses

# Variabel untuk menyimpan kalimat Wikipedia yang sudah ditokenisasi
wikipedia_tokenized_sentences = []

# --- 2. MENANGANI NAMESPACE ---
namespaces = {'mw': 'http://www.mediawiki.org/xml/export-0.11/'}  # Namespace untuk XML Wikipedia

# Fungsi pembersihan teks
def clean_text(text):
    text = text.lower()
    text = re.sub(r'<doc[^>]*>', '', text)
    text = re.sub(r'</doc>', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Memproses file XML secara streaming
def process_wikipedia_xml_in_streaming(wikipedia_xml_dump_path):
    # Membaca XML dengan iterparse (streaming)
    context = ET.iterparse(wikipedia_xml_dump_path, events=("start", "end"))

    # Mengambil root dari file XML
    _, root = next(context)

    for event, elem in context:
        if event == "start" and elem.tag == '{http://www.mediawiki.org/xml/export-0.11/}page':
            # Mengambil title dengan pengecekan None
            title_elem = elem.find('{http://www.mediawiki.org/xml/export-0.11/}title', namespaces)
            text_elem = elem.find('.//{http://www.mediawiki.org/xml/export-0.11/}text', namespaces)

            # Jika elemen title dan text ada
            if title_elem is not None and text_elem is not None:
                title = title_elem.text
                text = text_elem.text

                if text:
                    cleaned_text = clean_text(text)
                    # Pisahkan artikel menjadi kalimat dan tokenisasi
                    for sentence in cleaned_text.split('\n'):
                        if sentence.strip():  # Hanya memproses kalimat yang tidak kosong
                            wikipedia_tokenized_sentences.append(sentence.strip().split())

            # Setelah diproses, clear elemen untuk mengurangi penggunaan memori
            root.clear()

# Memproses file XML secara streaming dengan namespace
print("Memulai pemrosesan file XML Wikipedia secara streaming dengan namespace...")
process_wikipedia_xml_in_streaming(wikipedia_xml_dump_path)
print(f"Berhasil mengekstrak {len(wikipedia_tokenized_sentences)} kalimat dari file XML Wikipedia.")

# --- 3. SIMPAN KORPUS WIKIPEDIA YANG TELAH DITOKENISASI ---
if wikipedia_tokenized_sentences:
    print(f"Menyimpan korpus Wikipedia yang sudah ditokenisasi ke {processed_wikipedia_corpus_file}...")
    with open(processed_wikipedia_corpus_file, 'w', encoding='utf-8') as f_out:
        for tokens in wikipedia_tokenized_sentences:
            f_out.write(' '.join(tokens) + '\n')
    print("Penyimpanan selesai.")
else:
    print("Tidak ada data Wikipedia yang berhasil diproses untuk disimpan.")

# --- 4. LATIH MODEL WORD2VEC PADA KORPUS WIKIPEDIA ---
print("Memulai pelatihan model Word2Vec pada korpus Wikipedia yang sudah diproses...")
model_w2v_enhanced = Word2Vec(
    sentences=wikipedia_tokenized_sentences,
    vector_size=300,   # Ukuran vektor kata
    window=5,          # Jendela kata untuk pelatihan
    min_count=5,       # Kata yang muncul kurang dari 5 kali diabaikan
    workers=4,         # Jumlah thread untuk melatih
    sg=0               # Gunakan CBOW (sg=0), jika sg=1 maka menggunakan Skip-gram
)

# --- 5. SIMPAN MODEL WORD2VEC ---
word2vec_model_path = "/content/drive/MyDrive/data_UAS_KCB/indonesian_word2vec_enhanced.model"
model_w2v_enhanced.save(word2vec_model_path)
print(f"Model Word2Vec yang sudah ditingkatkan disimpan di: {word2vec_model_path}")

# Opsional: Uji model Word2Vec dengan beberapa kata
try:
    test_word = 'indonesia'  # Ganti dengan kata yang relevan
    if test_word in model_w2v_enhanced.wv:
        print(f"\nKata yang paling mirip dengan '{test_word}':")
        print(model_w2v_enhanced.wv.most_similar(test_word, topn=5))
    else:
        print(f"\nKata '{test_word}' tidak ditemukan dalam vocabulary model Word2Vec.")
except Exception as e_test:
    print(f"Error saat menguji model: {e_test}")

# Tokenizing text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Padding sequences untuk memastikan ukuran input yang seragam
max_len = 55  # Panjang tweet maksimum yang teridentifikasi
X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')
print(X_train_pad.shape)
print(X_test_pad.shape)

# Load model Word2Vec (misalnya yang sudah dilatih sebelumnya)
word2vec_model = "/content/drive/MyDrive/data_UAS_KCB/indonesian_word2vec_enhanced.model"
max_len = 55
# Fungsi untuk mengonversi token menjadi vektor
def text_to_word_vectors(text, word2vec_model, max_len):
    # Inisialisasi vektor untuk tweet
    vectors = []
    for word in text:
        # Jika kata ada di dalam model, ambil vektornya
        if word in model_w2v_enhanced.wv:
            vectors.append(model_w2v_enhanced.wv[word])
        else:
            # Jika kata tidak ada, gunakan vektor nol
            vectors.append(np.zeros(300))

    # Batasi panjang tweet sesuai max_len
    vectors = vectors[:max_len]

    # Tambahkan padding jika tweet lebih pendek dari max_len
    while len(vectors) < max_len:
        vectors.append(np.zeros(300))

    return np.array(vectors)

# Mengonversi data training dan testing menjadi vektor
X_train_vec = np.array([text_to_word_vectors(tweet, word2vec_model, max_len) for tweet in X_train_seq])
X_test_vec = np.array([text_to_word_vectors(tweet, word2vec_model, max_len) for tweet in X_test_seq])

print(X_train_vec.shape)  # Harusnya (15952, 55, 300)
print(X_test_vec.shape)   # Harusnya (3988, 55, 300)

# Label Encoding
le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)

# Definisikan embedding matrix
embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 300)) # Asumsi vector_size Word2Vec adalah 300
for word, idx in tokenizer.word_index.items():
    try:
        # GANTI 'model_w2v_trained' dengan nama variabel model Word2Vec Anda yang benar
        embedding_vector = model_w2v_enhanced.wv[word]
        embedding_matrix[idx] = embedding_vector
    except KeyError:
        # Kata tidak ditemukan di Word2Vec vocab, biarkan vektornya nol
        continue

# Pastikan max_len sudah didefinisikan sebelumnya (misalnya dari sel ke-7 atau ke-9)
# Jika belum, atau untuk memastikan:
# max_len = 55 # Sesuaikan jika perlu, tapi harus konsisten dengan padding

embedding_layer = Embedding(
    input_dim=len(tokenizer.word_index) + 1,
    output_dim=300, # Harus sama dengan vector_size Word2Vec
    weights=[embedding_matrix],
    input_length=max_len, # Panjang sekuens input setelah padding
    trainable=False  # Bobot embedding tidak akan diupdate selama training LSTM
)

print("Embedding layer berhasil dibuat.")
print(f"Bentuk embedding matrix: {embedding_matrix.shape}")

# Membangun model LSTM
model = Sequential()
model.add(embedding_layer)
model.add(Dropout(0.5))
model.add(LSTM(128, activation='tanh'))
model.add(Dense(1, activation='sigmoid'))
model.summary()

# Menyusun model
model.compile(loss='binary_crossentropy',  # Loss function untuk klasifikasi biner
              optimizer=RMSprop(learning_rate=0.0001),  # Optimizer RMSprop dengan learning rate 0.0001
              metrics=['accuracy'])  # Metrics untuk evaluasi (akurasi)
model.summary()

# Melatih model
history = model.fit(X_train_pad, y_train_encoded,  # Data latih dan label
                    epochs=10,  # Jumlah epoch pelatihan
                    batch_size=32,  # Ukuran batch
                    validation_data=(X_test_pad, y_test_encoded))  # Data uji untuk validasi

# Evaluasi model
score = model.evaluate(X_test_pad, y_test_encoded, batch_size=128)
print(f"Test Loss: {score[0]}")
print(f"Test Accuracy: {score[1]}")

# Melakukan prediksi terhadap data uji
y_pred = model.predict(X_test_pad)

# Mengonversi prediksi ke dalam bentuk label biner (0 atau 1)
y_pred_labels = (y_pred > 0.5).astype("int32")  # 0.5 digunakan sebagai threshold untuk sigmoid

# Menghitung confusion matrix
cm = confusion_matrix(y_test_encoded, y_pred_labels)

# Menampilkan confusion matrix dalam bentuk heatmap
plt.figure(figsize=(6,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Melakukan prediksi terhadap data latih (training data)
y_train_pred = model.predict(X_train_pad)

# Mengonversi prediksi ke dalam bentuk label biner (0 atau 1)
y_train_pred_labels = (y_train_pred > 0.5).astype("int32")  # 0.5 digunakan sebagai threshold untuk sigmoid

# Menghitung confusion matrix untuk data latih
cm_train = confusion_matrix(y_train_encoded, y_train_pred_labels)

# Menampilkan confusion matrix dalam bentuk heatmap untuk data latih
plt.figure(figsize=(6,6))
sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.title('Confusion Matrix for Training Data')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

from sklearn.metrics import classification_report

# Mencetak classification report
print(classification_report(y_test_encoded, y_pred_labels))

# --- Sel untuk Mengklasifikasikan Inputan Baru ---

def predict_new_sentiment(text):
    """
    Fungsi untuk membersihkan, memproses, dan memprediksi sentimen dari teks baru.

    Args:
    text (str): Teks (seperti tweet) yang akan dianalisis.

    Returns:
    tuple: Label prediksi (str) dan skor probabilitasnya (float).
    """
    # 1. Bersihkan teks input menggunakan fungsi clean_text yang sama
    cleaned_text = clean_text(text)

    # 2. Ubah teks menjadi sekuens integer menggunakan tokenizer yang sudah di-fit
    sequence = tokenizer.texts_to_sequences([cleaned_text])

    # 3. Lakukan padding pada sekuens agar panjangnya sesuai (max_len)
    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')

    # 4. Gunakan model yang sudah dilatih untuk melakukan prediksi
    # Hasilnya adalah probabilitas, karena layer terakhir menggunakan sigmoid
    prediction_prob = model.predict(padded_sequence)

    # Ambil nilai probabilitas dari output
    probability_score = prediction_prob[0][0]

    # 5. Tentukan label berdasarkan ambang batas 0.5
    if probability_score > 0.5:
        label = "Positive"
    else:
        label = "Negative"

    return label, probability_score

# Contoh 1: Teks dengan sentimen cenderung positif
new_text_1 = "Kebijakan physical distancing sangat efektif, saya dukung penuh pemerintah."
predicted_label_1, score_1 = predict_new_sentiment(new_text_1)

print(f"Teks Input: '{new_text_1}'")
print(f"Hasil Prediksi: Sentimen {predicted_label_1}")
print(f"Skor Probabilitas (mendekati 1 = Positif): {score_1:.4f}\n")


# Contoh 2: Teks dengan sentimen cenderung negatif dan mengandung slang
new_text_2 = "Gara2 social distancing gw jadi ga bisa kerja, bener2 nyusahin bgt."
predicted_label_2, score_2 = predict_new_sentiment(new_text_2)

print(f"Teks Input: '{new_text_2}'")
print(f"Hasil Prediksi: Sentimen {predicted_label_2}")
print(f"Skor Probabilitas (mendekati 0 = Negatif): {score_2:.4f}\n")


# Contoh 3: Coba dengan input Anda sendiri
my_own_text = "Social Distancing ini bikin aku jadi scroll tiktok mulu wkwkw"
predicted_label_3, score_3 = predict_new_sentiment(my_own_text)

print(f"Teks Input: '{my_own_text}'")
print(f"Hasil Prediksi: Sentimen {predicted_label_3}")
print(f"Skor Probabilitas: {score_3:.4f}")